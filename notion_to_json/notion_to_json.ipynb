{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk ## natural language processing\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs ## parsing html\n",
    "import os ## access other files\n",
    "import json ## to save data in more accessible format\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## iterate over all files in MusicVocab and extract sentences\n",
    "## store sentences in a json file (key-value pairs)\n",
    "## begin co-occurence counting\n",
    "\n",
    "## EXTRACT PLAINTEXT function\n",
    "def ext_plaintxt(dir_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    extract all text from the html files in dir\n",
    "    Parameters\n",
    "    ----------\n",
    "    dir : str\n",
    "        Relative path to directory we want to get text from, ex: 'MusicVocab_Eng2'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dir = os.fsencode(dir_path) ## get directory object from os\n",
    "    except:\n",
    "        print(\"error reading path\")\n",
    "\n",
    "    plaintxt = [] ## make an empty list to store the text\n",
    "    vocab = [] ## make empty list to store page titles (controlled vocab)\n",
    "\n",
    "    for file in os.listdir(dir):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith('.html'): ## if it is an html file \n",
    "            filetxt = '' ## make empty str to store the file text\n",
    "            with open(dir_path + '/' + filename) as f: ## open file as 'f'\n",
    "                filetxt = f.read()\n",
    "\n",
    "            ## now to use beautiful soup to extract plaintext\n",
    "            ## from the html text\n",
    "            soup = bs(filetxt)\n",
    "            title = soup.find('h1').get_text()\n",
    "            tags = [ t.get_text() for t in soup.find_all('td span.selected_value') ]\n",
    "            textonly = ' '.join([ p.get_text() for p in  soup.find_all('p') ])\n",
    "            ## the above code produces the text of each p (aka paragraph) in one big text\n",
    "\n",
    "            ## add to our lists\n",
    "            vocab.append(title)\n",
    "            text_with_tags = {\n",
    "                'text': textonly,\n",
    "                'tags': tags\n",
    "            }\n",
    "\n",
    "            plaintxt.append(text_with_tags)\n",
    "\n",
    "    return dict(zip(vocab, plaintxt)) ## return a key-pair matching for all vocab words\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = ext_plaintxt('../../masterdatabase/')\n",
    "data_dump = json.dumps(alldata) ## prepare data for export\n",
    "\n",
    "with open('master_controlled_vocab.json', 'w') as f:\n",
    "    f.write(data_dump)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nets2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
